# 对词与文档频率进行分析：tf-idf {#tfidf}

```{r echo = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 100)
library(ggplot2)
theme_set(theme_light())
```

文本挖掘与自然语言处理的一个中心问题是如何量化一个文档的内容。我们可以通过观察组成文档的词做到这一点吗？一个词的重要性的测度可以是其 *词频*（term frequency，tf），一个词在一个文档中出现的频率，我们在章 \@ref(tidytext) 中已经检查过了。然而，一个文档中有些词出现了很多次但可能并不重要；在英语中，这些词很可能是诸如“the”“is”“of”之类。我们可以通过加入停止词列表的方法在分析前就把这些词去掉，但是这些词中的某些词可能在有的文档中比另一些更重要。用停止词列表调整常用词词频的方法还不够精巧。

另一种方法是观察一个术语在一组文档中的 *逆向文档频率*（inverse document frequency，idf），可降低常用词的权重并提高不很常用的词的权重。可以将 idf 与 tf 合并来计算一个术语的 *tf-idf*（把这两个量相乘），从而用一个术语不常见的量度来调整其频率。

```{block, type = "rmdnote"}
The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites. 
```

这是条经验法则，是启发性质的量化；尽管可以验证 tf-idf 在文本挖掘、搜索引擎等应用中有效，信息理论专家认为其理论基础尚薄弱。任意给定的术语的逆向文档频率定义为：

$$idf(\text{术语}) = \ln{\left(\frac{n_{\text{文档的数量}}}{n_{\text{含有术语的文档的数量}}}\right)}$$

如果上面的公式看不清楚，可以在其上点击右键，选择 `Math Settings -> Math Renderer -> HTML-CSS` 或其它合适的选项。有时候 MathJax 默认选项对中文支持不佳。

我们可以如同章 \@ref(tidytext) 中描述的那样使用 tidy 数据原则进行 tf-idf 分析，使用一致、有效的工具来量化不同术语对一组文档中的一个文档的重要程度。

## 《红楼梦》中的词频（tf）

从查看词频开始，然后才是 tf-idf。我们首先使用 dplyr 的功能，如 `group_by()` 和 `join()`。《红楼梦》中最常用的词有哪些？（为了后面计算 tf-idf，我们需要一组文档，因此把《红楼梦》按每二十章共分为六个部分。）

```{r chapter_words}
library(jiebaR)
library(dplyr)
library(stringr)
library(tidytext)

load("data/hongloumeng.rda")
cutter <- worker(bylines = TRUE)
hongloumeng$text <- sapply(segment(hongloumeng$text, cutter), function(x){paste(x, collapse = " ")})
chapter_words <- hongloumeng %>%
  mutate(linenumber = row_number(),
       chapter = paste("第",
                       1 + cumsum(str_detect(text, "^第[零一二三四五六七八九十百 ]*([二四六八 ]+十|零) ?一回")),
                       "部分")) %>%
  ungroup() %>%
  unnest_tokens(word, text) %>%
  count(chapter, word, sort = TRUE) %>%
  ungroup()
total_words <- chapter_words %>% 
  group_by(chapter) %>% 
  summarize(total = sum(n))

chapter_words <- left_join(chapter_words, total_words)

chapter_words
```

此 `chapter_words` 数据框中每个词-部分之组合一行；`n` 是该词在该部分中出现的次数，`total` 是该部分全部词的数量。有最高 `n` 值的通常怀疑对象显然包括了“了”和“的”等。在图 \@ref(fig:plottf)中，可以看到每部分中 `n/total` 的分布，一个词在一部分中出现的次数除以该部分所有术语（即词）的总量。这就是词频的含义。

```{r plottf, dependson = "chapter_words", fig.height=9, fig.width=9, fig.cap="《红楼梦》各部分的词频分布"}
library(showtext)
showtext_auto(enable = TRUE)
font_add("WenQuanYi Micro Hei", "data/wqy-microhei.ttc")

ggplot(chapter_words, aes(n/total, fill = chapter)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~chapter, ncol = 2, scales = "free_y") +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

每个部分的右侧都有很长的长尾（那些十分常见的词！），在图中并未显示。这些图显示了所有部分都有相似的分布，即很多词出现得不多而较少词出现很频繁。

## 齐夫定律（Zipf's law）

如图 \@ref(fig:plottf) 所示的分布在语言中很典型。实际上，给出任何自然语言的语料（如一本书，来自网站的大量文本，或是大量口述），长尾分布的类型都如此常见，因此一个词的频率与其排名的关系一直是研究的对象；这种关系的一个经典版本叫做齐夫定律，来自一位20世纪美国语言学家乔治·齐夫。

```{block, type = "rmdnote"}
Zipf's law states that the frequency that a word appears is inversely proportional to its rank. 
```

有了用来绘制词频的数据框，只要几行 dplyr 函数即可在《红楼梦》的各部分上检验齐夫定律。

```{r freq_by_rank, dependson = chapter_words}
freq_by_rank <- chapter_words %>% 
  group_by(chapter) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank
```

这里的 `rank` 列显示了频率表内每个词的排名；由于表已经按 `n` 排序，我们可以用 `row_number()` 来确定排名。之后，我们可以按与之前一样的方式计算词频。要可视化齐夫定律，通常以排名为横坐标，词频为纵坐标，均使用对数比例。这种方式得到的逆向比例关系图像有稳定的负斜率。

```{r zipf, dependson = "freq_by_rank", fig.width=6, fig.height=5, fig.cap="《红楼梦》中的齐夫定律"}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = chapter)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

注意，图 \@ref(fig:zipf) 中的坐标轴为对数-对数关系。可以看出，《红楼梦》的六个部分都很相似，排名与频率间的关系呈现负斜率，然而并不十分恒定。也许我们可以把这条线分成比方说三段，呈现为[幂定律](https://en.wikipedia.org/wiki/Power_law)。来看一下排名区间位于中段的幂指数如何。

```{r lower_rank, dependson = "freq_by_rank"}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```

经典版本的齐夫定律有

$$\text{频率} \propto \frac{1}{\text{排名}}$$
而我们实际上已经得到了接近 -1 的斜率。把拟合的幂定律绘制到图 \@ref(fig:zipffit) 里看看是什么样。

```{r zipffit, dependson = "freq_by_rank", fig.width=6, fig.height=5, fig.cap="带有拟合指数的《红楼梦》中的齐夫定律"}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = chapter)) + 
  geom_abline(intercept = -0.90, slope = -0.96, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

我们在《红楼梦》的语料中发现了和经典版本齐夫定律接近的结果。在高排名端的情况并不特别，很多语言都如此；语料中通常含有比单一幂定律的预测更少的不常见词。在低排名端的变化较不寻常。曹雪芹用得最多的词的占比低于很多其它语料。这种分析可以扩展到比较不同作家，或是比较不同的文本集合；用 tidy 数据原则很容易做到。

## The `bind_tf_idf` function

The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the group of Jane Austen's novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not *too* common. Let's do that now.

The `bind_tf_idf` function in the tidytext package takes a tidy text dataset as input with one row per token (term), per document. One column (`word` here) contains the terms/tokens, one column contains the documents (`book` in this case), and the last necessary column contains the counts, how many times each document contains each term (`n` in this example). We calculated a `total` for each book for our explorations in previous sections, but it is not necessary for the `bind_tf_idf` function; the table only needs to contain all the words in each document.

```{r tf_idf, dependson = "chapter_words"}
chapter_words <- chapter_words %>%
  bind_tf_idf(word, chapter, n)
chapter_words
```

Notice that idf and thus tf-idf are zero for these extremely common words. These are all words that appear in all six of Jane Austen's novels, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection. 

Let's look at terms with high tf-idf in Jane Austen's works.

```{r desc_idf, dependson = "tf_idf"}
chapter_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

Here we see all proper nouns, names that are in fact important in these novels. None of them occur in all of novels, and they are important, characteristic words for each text within the corpus of Jane Austen's novels. 

```{block, type = "rmdnote"}
Some of the values for idf are the same for different terms because there are 6 documents in this corpus and we are seeing the numerical value for $\ln(6/1)$, $\ln(6/2)$, etc. 
```

Let's look at a visualization for these high tf-idf words in Figure \@ref(fig:plotseparate).

```{r plotseparate, dependson = "plot_austen", fig.height=10, fig.width=9, fig.cap="Highest tf-idf words in each of Jane Austen's Novels"}
chapter_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(chapter) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = chapter)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~chapter, ncol = 2, scales = "free") +
  coord_flip() +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

Still all proper nouns in Figure \@ref(fig:plotseparate)! These words are, as measured by tf-idf, the most important to each novel and most readers would likely agree. What measuring tf-idf has done here is show us that Jane Austen used similar language across her six novels, and what distinguishes one novel from the rest within the collection of her works are the proper nouns, the names of people and places. This is the point of tf-idf; it identifies words that are important to one document within a collection of documents.

## Summary 

Using term frequency and inverse document frequency allows us to find words that are characteristic for one document within a collection of documents, whether that document is a novel or physics text or webpage. Exploring term frequency on its own can give us insight into how language is used in a collection of natural language, and dplyr verbs like `count()` and `rank()` give us tools to reason about term frequency. The tidytext package uses an implementation of tf-idf consistent with tidy data principles that enables us to see how different words are important in documents within a collection or corpus of documents.

