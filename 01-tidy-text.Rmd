# tidy 文本格式 {#tidytext}

```{r echo = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)
options(width = 100, dplyr.width = 100)
library(ggplot2)
theme_set(theme_light())
```

处理数据的 tidy 数据原则简单有效，用于文本也一样。按 Hadley Wickham [@tidydata] 的阐述，tidy 数据有如下特定的结构：

* 每个 variable 一列
* 每个 observation 一行
* 每种 observational unit 一个表格

于是，我们不妨定义 tidy 文本格式为一个 **每行一个符号的表格**。一个符号（token）是文本的一个有意义的单元，比如我们在分析中经常使用的词，而符号化（tokenization）就是将文本切分为符号的过程。这种“每行一个符号”的结构与当下分析中常用的其它文本存储方式形成鲜明对比，如字符串或者文档-术语矩阵。用于 tidy 文本挖掘时，存储在每行的 **符号** 通常是单个词，但也可以是 n元语（n-gram）、句或段落。在 tidytext 包里提供了符号化（tokenize）这些常见单元的方法，将其转换至“每项一行”的格式。

Tidy 数据集可以使用一组标准的 “tidy” 工具进行操作，包括了流行的包如 dplyr [@R-dplyr]、tidyr [@R-tidyr]、ggplot2 [@R-ggplot2] 和 broom [@R-broom]。若能保持输入和输出均为 tidy 表格，用户可以在这些包之间自如转换。我们发现这些 tidy 工具可以自然地拓展到很多文本分析和探索活动中。

与此同时，tidytext 包并不强制用户在一次分析中全程保持文本数据为 tidy 形式，而是包含了 `tidy()` 多种对象的函数（见 broom 包），可以来自流行的文本挖掘 R 包，如 tm [@tm] 和 quanteda [@R-quanteda]。这使得类似如下的工作流成为可能：使用 dplyr 和其它 tidy 工具对数据进行导入、过滤和处理，之后转换为文档-术语矩阵进行机器学习，所得到的模型可以再被转换回 tidy 形式供解读并由 ggplot2 视觉化。

## tidy 文本与其它数据结构的对比

如前所述，我们定义 tidy 文本格式为一个表格，**每行一个符号**。以这种方式结构化的文本数据遵循 tidy 数据原则，可以用统一一致的工具进行操作。这值得和文本挖掘方法中常用的其它存储文本的格式进行对比。

* **字符串**：文本当然可以用字符串存储，也就是 R 中的字符向量，这种形式的文本数据一般会先读进内存。
* **语料**：这些种类的典型对象包含了原始的字符串，并带有额外的元数据和细节标注等。
* **文档-术语矩阵**：这是一个稀疏矩阵，描述了文档的一个集合（即一组语料），每个文档一行，每个术语一列。矩阵中的典型数据为词的个数或 tf-idf（见章 \@ref(tfidf)）。

咱们先暂时不去探索语料和文档-术语矩阵对象，章 \@ref(dtm) 将会讲到这些。从将文本转换成 tidy 格式的基础开始。

## `unnest_tokens` 函数

英文原著使用了Emily Dickinson的一首小诗，这里选取李白的《静夜思》作为中文的例子。

```{r text}
text <- c("床前明月光，",
          "疑是地上霜。",
          "举头望明月，",
          "低头思故乡。")
text
```

中文一般不用空格隔开词，所以需要完成分词的步骤（结果并不完全准确）。这里选择 [jiebaR](https://github.com/qinwf/jiebaR) [@R-jiebaR]，内置了多种分词方式并直接支持停止词等。

```{r text_wb, dependson = "text"}
library(jiebaR)
cutter <- worker(bylines = TRUE, symbol = TRUE)
text_wb <- sapply(segment(text, cutter), function(x){
  paste(x, collapse = " ")})
text_wb
```

我们想要分析的是个典型的字符向量。要把它变成 tidy 文本数据集，我们先要把它放进数据框。

```{r text_df, dependson = "text_wb"}
library(dplyr)
text_df <- data_frame(line = 1:4, text = text_wb)
text_df
```

这个数据框显示为”tibble“是什么意思？一个 tibble 是 R 里一个现代的数据框类，在 dplyr 和 tibble 包中可用，它有个方便的打印方法，不把字符串转换为因子，也不为行命名。Tibble 特别适用于 tidy 工具。

注意，这个包含文本的数据框尚未兼容 tidy 文本分析。我们不能过滤出词，也不能计数哪些出现更频繁，因为每行都由多个词合并组成。我们需要把它转换成 **每行每文档一个符号**。 

```{block, type = "rmdnote"}
A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens. 
```

在第一个例子里，我们只有一个文档（一首诗），不过我们马上就会探索多个文档的例子。

在我们的 tidy 文本框架中，我们需要把文本拆分成独个的符号（这个过程叫做 *符号化*）*并且* 将之变形为 tidy 数据结构。要做到这些，可以使用 tidytext 的 `unnest_tokens()` 函数。特意为中文用户解释一下词源，un-nest 即 nest 的反向操作：把内容从 nest 里取出来。

```{r dependson = "text_df", R.options = list(dplyr.print_max = 10)}
library(tidytext)
text_df %>%
  unnest_tokens(word, text)
```

这里用到 `unnest_tokens` 的两个基本参数是列名。首先是将要创建的输出的列名，文本将被拆分到这里面（在这个例子里是 `word`），然后是输入的列名，文本来自于此（在这个例子里是 `text`）。回忆一下，上面的 `text_df` 叫做 `text` 的列包含了所需的数据。

用过 `unnest_tokens` 之后，我们把每行拆分了，于是现在新的数据框里每行有一个符号（词）；正如所见，`unnest_tokens()` 默认按单个词进行符号化。还需要注意：

* 其它列原样保留，比如每个词来自的行的行号。
* 标点会被去掉。
* 默认情况下，`unnest_tokens()` 把符号转换为小写字符，这是为了更方便与其它数据集比较或合并（用 `to_lower = FALSE` 参数可关闭这个行为）。

有了这种格式的数据，我们可以使用标准的 tidy 工具套装进行操作、处理和可视化，即 dplyr、tidyr 和 ggplot2。如图 \@ref(fig:tidyflow-ch1) 所示。

```{r tidyflow-ch1, echo = FALSE, out.width = '100%', fig.cap = "A flowchart of a typical text analysis using tidy data principles. This chapter shows how to summarize and visualize text using these tools."}
knitr::include_graphics("images/tidyflow-ch-1.png")
```

## tidy 化一些伟大的作品 {#tidyworks}

英文的例子可使用 [janeaustenr](https://cran.r-project.org/package=janeaustenr) [@R-janeaustenr] 引入 Jane Austen 的六部完整已发表小说的文本，并可转换为 tidy 格式。janeaustenr 包提供的文本格式每行为书页里的一行，这个行严格对应着实体书里印刷的行。咱们从这里开始，依旧使用 `mutate()` 添加 `linenumber` 批注以记录原始格式里的行数，并且添加 `chapter` 批注（使用正则表达式）以找到所有的章节位置。

```{r original_books}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
original_books
```

要把这个当成 tidy 数据集，我们需要重构为每行一个符号的格式，早些时候我们看到可以用 `unnest_tokens()` 函数完成这个操作。

```{r tidy_books_raw, dependson = "original_books"}
library(tidytext)
tidy_books <- original_books %>%
  unnest_tokens(word, text)
tidy_books
```

这个函数使用 [tokenizers](https://github.com/ropensci/tokenizers) [R-tokenizers] 包把原始数据框里的每行文本按符号分开。默认按词符号化，而其它选项包括了字符、n元组、句、行、段落，以及正则表达式。

现在数据已经是每行一词的格式，我们可以用 tidy 工具如 dplyr 进行操作了。对于文本分析，我们经常想要移除停止词；停止词是对分析没有帮助的词，典型的停止词包括极度常见的词，如英文里的 "the"、"of"、"to"等等。我们可以用 `anti_join()` 移除停止词（tidytext 的数据集里存有 `stop_words`）。

```{r tidy_books, dependson = "tidy_books_raw"}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)
```

tidytext 包里的 `stop_words` 数据集包含有分别来自三个词典的停止词。我们可以一起使用，就像上面这样，或用 `filter()` 来选择对特定的分析更合适的一个停止词数据集。

我们还可以用 dplyr 的 `count()` 找出所有书作为一个整体最常见的词。

```{r dependson = "tidy_books"}
tidy_books %>%
  count(word, sort = TRUE) 
```

因为我们一直在使用 tidy 工具，词的个数存在一个 tidy 数据框里。这让我们可以把它直接通过管道传给 ggplot2 包，例如创建一个最常见的词的可视化（图 \@ref(fig:plotcount)）。

```{r plotcount, dependson = "tidy_books", fig.width=6, fig.height=5, fig.cap="The most common words in Jane Austen's novels"}
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

需要注意的是， `austen_books()` 函数恰好提供给了我们想要分析的数据，但其它情况下我们可能需要对文本数据进行清洗，比如去除版权信息或重新格式化。在案例分析的章节里将看到这种预处理的例子，特别是章 \@ref(pre-processing-text)。

中文暂时缺少类似 janeaustenr 这样的数据集，我们可以从《红楼梦》的部分文本开始。

```{r great_works}
library(dplyr)
library(stringr)

great_works <- data_frame(text = readLines("data/hongloumeng/001.txt"), chapter = "第一回") %>%
  mutate(linenumber = row_number(), chapter = 1)
great_works
```

移除过于常见的停止词并分词。注意和第一个例子的区别，这次我们没有保留标点符号（即使保留了也会在下一步被移除）。另外，由于中英文的不同以及包的具体实现有差别，我们直接在分词的阶段就把停止词移除了。

```{r tidy_works, dependson = "great_works"}
library(jiebaR)
library(tidytext)

cutter <- worker(bylines = TRUE, stop_word = "data/stop_word_zh.utf8")
great_works$text <- sapply(segment(great_works$text, cutter), function(x){paste(x, collapse = " ")})
tidy_works <- great_works %>%
  unnest_tokens(word, text)
tidy_works
```

按词频进行排序。

```{r dependson = "tidy_works"}
tidy_works %>%
  count(word, sort = TRUE)
```

图 \@ref(fig:plotcount-zh) 展示了常见词词频，由于本次使用文本较少，词的数量并不大。这里尝试使用 [showtext](https://github.com/yixuan/showtext) [@R-showtext] 方便图片里的中文字符正确渲染而无需依赖系统字体。[文泉驿微米黑](https://wenq.org/wqy2/index.cgi?MicroHei)是自由字体，遵循[GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html)，允许所有人复制和发布。

```{r plotcount-zh, dependson = "tidy_works", fig.width=6, fig.height=5, fig.cap="《红楼梦》第一回常见词"}
library(showtext)
showtext_auto(enable = TRUE)
font_add("WenQuanYi Micro Hei", "data/wqy-microhei.ttc")

tidy_works %>%
  count(word, sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

## gutenbergr 包

现在我们用过 janeaustenr 包探索如何 tidy 文本，下面介绍一下 [gutenbergr](https://github.com/ropensci/gutenbergr) 包 [@R-gutenbergr]。使用 gutenbergr 包可访问来自[古登堡计划](https://www.gutenberg.org/)的公共领域作品集。包中有下载书籍的工具（且去除了用不上的头部、尾部信息），还有一份完整的古登堡计划数据集元数据，可以用来寻找感兴趣的作品。在这本书中，我们主要使用 `gutenberg_download()` 函数通过ID从古登堡计划下载一部或多部作品，不过也可使用其它函数来探索元数据，将古登堡ID与作品名、作者、语言等匹配，或是收集有关作者的信息。我们要使用的中文作品就是在如下数据中选出的。

```{r gutenberg-zh}
library(gutenbergr)

gutenberg_works(languages = "zh")
```

```{block, type = "rmdtip"}
To learn more about gutenbergr, check out the [package's tutorial at rOpenSci](https://ropensci.org/tutorials/gutenbergr_tutorial.html), where it is one of rOpenSci's packages for data access.
```

## Word frequencies

A common task in text mining is to look at word frequencies, just like we have done above for Jane Austen's novels, and to compare frequencies across different texts. We can do this intuitively and smoothly using tidy data principles. We already have Jane Austen's works; let's get two more sets of texts to compare to. First, let's look at some science fiction and fantasy novels by H.G. Wells, who lived in the late 19th and early 20th centuries. Let's get [*The Time Machine*](https://www.gutenberg.org/ebooks/35), [*The War of the Worlds*](https://www.gutenberg.org/ebooks/36), [*The Invisible Man*](https://www.gutenberg.org/ebooks/5230), and [*The Island of Doctor Moreau*](https://www.gutenberg.org/ebooks/159). We can access these works using `gutenberg_download()` and the Project Gutenberg ID numbers for each novel.

中文我们可以使用冯梦龙的《三言》和袁枚《子不语》《续子不语》分别与《红楼梦》进行比较。冯梦龙[明]时间较早，而曹雪芹[清]与袁枚[清]处于同一时代，但题材上《红楼梦》与《三言》更为接近。这些作品间的相似程度将如何呢？

```{r eval = FALSE}
library(gutenbergr)

hongloumeng <- gutenberg_download(24264)
sanyan <- gutenberg_download(c(24141, 24239, 27582))
zibuyu <- gutenberg_download(c(25245, 25315))
```

这次我们使用《红楼梦》的全文。

```{r hongloumeng, echo = FALSE}
load("data/hongloumeng.rda")
```

```{r tidy_hongloumeng, dependson = "hongloumeng"}
library(jiebaR)

cutter <- worker(bylines = TRUE, stop_word = "data/stop_word_zh.utf8")
hongloumeng$text <- sapply(segment(hongloumeng$text, cutter), function(x){paste(x, collapse = " ")})
tidy_hongloumeng <- hongloumeng %>%
  unnest_tokens(word, text)
```

```{r dependson = "tidy_hongloumeng"}
tidy_hongloumeng %>%
  count(word, sort = TRUE)
```

简单试一下，看看冯梦龙这些小说里最常用的词是什么。

```{r sanyan, echo = FALSE}
load("data/sanyan.rda")
```

```{r tidy_sanyan, dependson = "sanyan"}
library(jiebaR)

cutter <- worker(bylines = TRUE, stop_word = "data/stop_word_zh.utf8")
sanyan$text <- sapply(segment(sanyan$text, cutter), function(x){paste(x, collapse = " ")})
tidy_sanyan <- sanyan %>%
  unnest_tokens(word, text)
```

```{r dependson = "tidy_sanyan"}
tidy_sanyan %>%
  count(word, sort = TRUE)
```

《子不语》两部依样处理。

```{r zibuyu, echo = FALSE}
load("data/zibuyu.rda")
```

```{r tidy_zibuyu, dependson = "zibuyu"}
library(jiebaR)

cutter <- worker(bylines = TRUE, stop_word = "data/stop_word_zh.utf8")
zibuyu$text <- sapply(segment(zibuyu$text, cutter), function(x){paste(x, collapse = " ")})
tidy_zibuyu <- zibuyu %>%
  unnest_tokens(word, text)
```

```{r dependson = "tidy_zibuyu"}
tidy_zibuyu %>%
  count(word, sort = TRUE)
```

看起来“人”在所有作品里都是最常见的。

Now, let's calculate the frequency for each word for the works of Jane Austen, the Brontë sisters, and H.G. Wells by binding the data frames together. We can use `spread` and `gather` from tidyr to reshape our dataframe so that it is just what we need for plotting and comparing the three sets of novels.

```{r frequency, dependson = c("tidy_hongloumeng", "tidy_sanyan", "tidy_zibuyu")}
library(tidyr)

frequency <- bind_rows(mutate(tidy_zibuyu, author = "袁枚"),
                       mutate(tidy_sanyan, author = "冯梦龙"),
                       mutate(tidy_hongloumeng, author = "Cao")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `袁枚`:`冯梦龙`)
```

We use `str_extract()` here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (like italics). The tokenizer treated these as words, but we don't want to count "\_any\_" separately from "any" as we saw in our initial data exploration before choosing to use `str_extract()`. 

Now let's plot (Figure \@ref(fig:plotcompare)).

```{r plotcompare, dependson = "frequency", fig.width=10, fig.height=5.5, fig.cap="比较曹雪芹、冯梦龙与袁枚作品的词频"}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Cao`, color = abs(`Cao` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, family = "WenQuanYi Micro Hei") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "曹雪芹", x = NULL) +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

Words that are close to the line in these plots have similar frequencies in both sets of texts, for example, in both Austen and Brontë texts ("miss", "time", "day" at the upper frequency end) or in both Austen and Wells texts ("time", "day", "brother" at the high frequency end). Words that are far from the line are words that are found more in one set of texts than another. For example, in the Austen-Brontë panel, words like "elizabeth", "emma", and "fanny" (all proper nouns) are found in Austen's texts but not much in the Brontë texts, while words like "arthur" and "dog" are found in the Brontë texts but not the Austen texts. In comparing H.G. Wells with Jane Austen, Wells uses words like "beast", "guns", "feet", and "black" that Austen does not, while Austen uses words like "family", "friend", "letter", and "dear" that Wells does not.

Overall, notice in Figure \@ref(fig:plotcompare) that the words in the Austen-Brontë panel are closer to the zero-slope line than in the Austen-Wells panel. Also notice that the words extend to lower frequencies in the Austen-Brontë panel; there is empty space in the Austen-Wells panel at low frequency. These characteristics indicate that Austen and the Brontë sisters use more similar words than Austen and H.G. Wells. Also, we see that not all the words are found in all three sets of texts and there are fewer data points in the panel for Austen and H.G. Wells.

Let's quantify how similar and different these sets of word frequencies are using a correlation test. How correlated are the word frequencies between Austen and the Brontë sisters, and between Austen and Wells?

```{r cor_test, dependson = "frequency"}
cor.test(data = frequency[frequency$author == "冯梦龙",],
         ~ proportion + `Cao`)
cor.test(data = frequency[frequency$author == "袁枚",],
         ~ proportion + `Cao`)
```

Just as we saw in the plots, the word frequencies are more correlated between the Austen and Brontë novels than between Austen and H.G. Wells.

## Summary

In this chapter, we explored what we mean by tidy data when it comes to text, and how tidy data principles can be applied to natural language processing. When text is organized in a format with one token per row, tasks like removing stop words or calculating word frequencies are natural applications of familiar operations within the tidy tool ecosystem. The one-token-per-row framework can be extended from single words to n-grams and other meaningful units of text, as well as to many other analysis priorities that we will consider in this book.
